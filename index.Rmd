---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Morgan Duran (mrd2926)

### Introduction 

The dataset used for this project is created from a combination of two datasets, "winequality-red" and "winequality-white", which each use 11 numeric measures to describe chemical qualities of wine, such as pH, sulphates, chlorides, etc. These two datasets measure the qualities of wine using the same set of characteristics and also provide a "quality score" based on the experience of consumtion. In the combination of these two datasets, a binary variable was created to distinguish the red wines from the white wines. 

I selected these datasets for use in this project with the hopes of identifying key chemical aspects of red vs. white wine and high vs. low quality wine. As a biochemistry major at UT, I nerd out over real-world chemistry applications and hoped to learn more about the interpretations of the chemical composition of wine on its perceived quality. Additionally, having grown up in Northern California near wine country, I was raised around many conversations about wine but never fully understood the implications of a wine having a high or low content of some chemical component.

This data was found from the UCI Machine Learning Repository, and can be accessed via the links below:

https://archive.ics.uci.edu/ml/datasets/Wine+Quality

https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/

```{R, warning=FALSE,message=FALSE}
library(tidyverse)
red <- read_csv("winequality-red.csv")
white <- read_csv("winequality-white.csv")
```

The tidying & binding of the two original datasets is completed in the code chunk below. The column "type" is created to distinguish instances of red vs. white wine, and all multi-word columns are renamed for ease of use.

```{R, warning=FALSE,message=FALSE}
library(dplyr)
red <- red %>% 
  mutate(type = "red") %>%
  rename(fixed_acidity = `fixed acidity`) %>%
  rename(volatile_acidity = `volatile acidity`) %>%
  rename(citric_acid = `citric acid`) %>%
  rename(residual_sugar = `residual sugar`) %>%
  rename(free_sulfur_dioxide = `free sulfur dioxide`) %>%
  rename(total_sulfur_dioxide = `total sulfur dioxide`)

white <- white %>% 
  mutate(type = "white") %>%
  rename(fixed_acidity = `fixed acidity`) %>%
  rename(volatile_acidity = `volatile acidity`) %>%
  rename(citric_acid = `citric acid`) %>%
  rename(residual_sugar = `residual sugar`) %>%
  rename(free_sulfur_dioxide = `free sulfur dioxide`) %>%
  rename(total_sulfur_dioxide = `total sulfur dioxide`)
  
all_wines <- rbind(red, white)
```

### Cluster Analysis

PAM clustering is performed on the combined all_wines dataset, using all 12 numeric variables present.

```{r}
library(cluster)
# clustering code here

clust_data <- all_wines %>% select(-type)

# picking number of clusters based on sil width
sil_width <- vector()
for (i in 2:10) {
    pam_fit <- pam(clust_data, diss = TRUE, k = i)
    sil_width[i] <- pam_fit$silinfo$avg.width
}

library(ggplot2)
ggplot() + geom_line(aes(x = 1:10, y = sil_width)) + 
    scale_x_continuous(name = "k", breaks = 1:10)
```

Based on the chart above, it appears that the best choice for number of clusters based on largest silhouette width is 2 clusters. 

```{R}
pam1 <- clust_data %>% pam(k=2)
pam1$silinfo$avg.width
plot(pam1)
```

This decision to use 2 clusters is further examined by looking at the average silhouette width of PAM clustering performed on the data with k=2 clusters. The average silhouette width of 0.52 suggests a reasonable structure has been found. In the clusterplot of this PAM clustering, the two clusters are somewhat clear, though they are fairly intertwined and not particularly separated from each other. PC1 and PC2 combined only explain ~47% of variance, which alone is insufficient to see proper clustering, perhaps explaining why the plot appears so muddied.

Visualization of clusters by pairwise combinations is shown below:

```{R fig.asp=1.5, fig.width=15}
library(GGally)
pamclust<-clust_data%>%mutate(cluster=as.factor(pam1$clustering))
ggpairs(pamclust, columns=1:12, aes(color=cluster))
```

While the two-component cluster plot created from the PAM analysis didn't create the clearest clusters, this pairwise combination shows some interesting results. Pairwise combinations with high correlations include: free sulfur dioxide & residual sugar (0.403), total sulfur dioxide & citric acid (-0.414), total sulfur dioxide & residual sugar (0.495), density & fixed acidity (0.459), density & residual sugar (0.553), and quality & alcohol (0.444). Total sulfur dioxide has the most separated peaks, and created the most distinct clusters when paired with all other variables. As expected, pH, when paired with measures of acidity, also created clearer clusters. Boths measures of acidity (fixed & volatile) also created somewhat clearer clusters when paired with all other variables. I would have expected components such as citric acid, chlorides, or sulphates to create more distinct clusters. I was not surprised that density, for the most part, did not create distinct clusters or have a large separation in its peaks.
    
    
### Dimensionality Reduction with PCA

PCA is performed on all 12 numeric variables in the all_wines dataset.

```{R}
wine_nums <- all_wines %>% select_if(is.numeric) %>% scale %>% as.data.frame
wine_pca <- princomp(wine_nums, cor=T)

summary(wine_pca, loadings=T)
```

The PC scores are visualized in a scree plot in order to help inform the decision of how many PCs to retain.

```{R}
eigval <-  wine_pca$sdev^2 #square to convert SDs to eigenvalues
varprop=round(eigval/sum(eigval), 2) #proportion of var explained by each PC

round(cumsum(eigval)/sum(eigval), 2) 

ggplot() + geom_bar(aes(y=varprop, x=1:12), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:12)) + 
  geom_text(aes(x=1:12, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + 
  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + 
  scale_x_continuous(breaks=1:10)
```

There are 3 "rules of thumb" discussed to determine how many PCs to retain: (1) pick PCs until the scree plot flattens, (2) pick PCs until cumulative proportion of variance is > 80%, (3) pick PCs whose eigenvalues are greater than 1 (Kaiser's rule). 

Following the first rule of thumb and examining the scree plot, flattening appears to begin around PC 5, so 4 PCs would be retained. Following the second rule of thumb and picking PCs until at least 80% of variance is accounted for, 6 PCs are retained. Following the third rule of thumb and selecting PCs with eigenvalues greater than 1, 4 PCs are retained.

In practice in labs and assignments in this course, the second rule of thumb was the one most frequently followed, so I would choose to retain 6 PCs. 

In the examination of the meanings of each of the PCs retained, the loadings/coefs maginitude and sign are looked at. PC1 is a *volatile acidity vs. total sulfur dioxide* axis, with a high score indicating high volatile acidity and a low score indicating high total sulfur dioxide. PC2 is a *density vs. alcohol* axis, with a high score indicating a high density and a low score indicating a high alcohol content. PC3 is a *citric acid vs. pH* axis, with a high score indicating a high citric acid content and a low score indicating a high pH. PC4 is a *fixed acidity vs. sulphates* axis, with a score indicating high fixed acidity and a low score indicating high sulphates. PC5 is a *residual sugar / quality vs. chlorides* axis, with a high score indicating high residual sugars and high quality and a low score indicating high chlorides. PC6 is a *pH vs. chlorides* axis, with a high score indicating high pH and a low score indicating high chlorides.


###  Linear Classifier

The dataset used is manipulated slightly to create a column to be used for the following classifiers and predictions. The binary variable "type" is altered such that type "red" is equivalent to the positive case (red -> 1) and type "white" is equivalent to the negative (white -> 0).

```{R}
red_bin <- red %>% mutate(type = 1)
white_bin <- white %>% mutate(type = 0)

all_wines_binary <- rbind(red_bin, white_bin)

all_wines_binary <- all_wines_binary %>% rename(y = type)

y <- all_wines_binary$y
y <- factor(y, levels=c("1", "0"))
```


Linear classifier code is shown in the code block below. A generalized linear model is used as the linear classifier to predict the binary response of the variable "type", which distinguishes red vs. white wine. These results are predicted from all 12 numeric variables in the all_wines_binary dataset. This model is trained to the entire all_wines_binary dataset.

```{R}
fit<-glm(y~., data=all_wines_binary, family="binomial")
coef(fit)

probs<-predict(fit,type="response") #get predicted probabilities
class_diag(probs,all_wines_binary$y, positive=1) 
```

A confusion matrix is reported below, where "1" continues to represent "red" and "0" continues to represent "white".

```{R}
y_hat <- sample(c("1","0"), size=length(y), replace=T)
y_hat <- factor(y_hat, levels=c("1","0"))

table(actual = y, predicted = y_hat) %>% addmargins
```

k-fold cross-validation of the linear classifier is performed in the code block below:

```{R}
set.seed(1234)
k=10 #choose number of folds

data<-all_wines_binary[sample(nrow(all_wines_binary)),] #randomly order rows
folds<-cut(seq(1:nrow(all_wines_binary)), breaks=k, labels=F) #create 10 folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data[folds!=i,]
  test<-data[folds==i,]
  truth<-test$y
  
  ## Train model on training set
  fit<-glm(y~., data=train, family="binomial")
  probs<-predict(fit, newdata = test, type="response")
  
  ## Test model on test set (save all k results)
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}

summarize_all(diags, mean)
```

The reported AUC of the initial linear classifier is 0.9961. The reported AUC of the cross-validation linear classifier is 0.9956. Both these AUC values are very good, suggesting that this model is very good at predicting our data. The similarity in the two AUC values (negligible drop-off from the model trained to the original dataset to the cross-validation model) suggests that there was not overfitting in the original model. 


### Non-Parametric Classifier

Non-parametric classifier code is shown in the code block below. A k-nearest-neighbors model is used as the non-parametric classifier in this section, first to train the model to the entire all_wines_binary dataset, and then for cross-validation of this model.

```{R}
library(caret)

fit <- knn3(y~., data=all_wines_binary)
probs<-predict(fit, newdata=all_wines_binary)[,2] #get predicted probabilities
class_diag(probs, all_wines_binary$y, positive=1) #get classification diagnostics
```

k-fold cross-validation of the non-parametric classifier is performed in the code block below:

```{R}
set.seed(1234)
k=10 #choose number of folds

data<-all_wines_binary[sample(nrow(all_wines_binary)),] #randomly order rows
folds<-cut(seq(1:nrow(all_wines_binary)),breaks=k,labels=F) #create 10 folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$y
  
  ## Train model on training set
  fit<-knn3(y~.,data=train)
  probs<-predict(fit,newdata = test)[,2]
  
  ## Test model on test set (save all k results)
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}

summarize_all(diags,mean)
```

The AUC of full-data is 0.9932. The AUC of the cross-validation trained model is 0.9652. While there is more differentiation between these two AUC instances than seen in the previous section (linear classifier), both these AUC values are still great. However, this drop in AUC between the non-parametric classifier fit to the whole dataset and the cross-validation model may suggest that there is some overfitting in the model trained to the original dataset. 


### Regression/Numeric Prediction

Regression model code block is shown below. A linear regression model is used in this section to predict the numeric values of "quality" from the other numeric variables.

```{R}
fit<-lm(quality~.,data=all_wines_binary) #predict quality from all other variables
yhat<-predict(fit)

# MSE
mean((all_wines_binary$quality-yhat)^2)
```

k-fold cross-validation of the regression model is performed below:

```{R}
set.seed(1234)
k=5 #choose number of folds

data<-all_wines_binary[sample(nrow(all_wines_binary)),] #randomly order rows
folds<-cut(seq(1:nrow(all_wines_binary)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  ## Fit linear regression model to training set
  fit<-lm(quality~., data=train)
  ## Get predictions/y-hats on test set (fold i)
  yhat<-predict(fit,newdata=test)
  ## Compute prediction error (MSE) for fold i
  diags<-mean((all_wines_binary$quality-yhat)^2)
}

mean(diags) 
```

The mean squared error (MSE) of the model fit to the entire dataset is 0.5364. The MSE of the out-of-sample cross-validation is 0.9860. MSE is a measure of prediction error, so it is better the smaller this value is. The MSE is higher in the cross-validation than in model fit to the original data, which suggests overfitting.


### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3", required = F)

df = all_wines
df <- df %>% filter(quality >=7) %>% arrange(desc(quality))

highest_quality <- df %>% filter(quality >= 9) 
```

```{python}
for qual in r.highest_quality:
  print(qual, end=": ")
  print(r.highest_quality[qual])
  
fixed_acidity = r.highest_quality['fixed_acidity']
pH = r.highest_quality['pH']
sulphates = r.highest_quality['sulphates']
alcohol = r.highest_quality['alcohol']
```

```{r}
py$sulphates
mean(py$fixed_acidity)
mean(py$sulphates)

mean(py$pH)
mean(py$pH)  - mean((white %>% filter(quality < 9))$pH)
mean(py$pH) - mean((all_wines %>% filter(quality < 9))$pH)

mean(py$alcohol)
mean(py$alcohol) - mean((white %>% filter(quality < 9))$alcohol)
mean(py$alcohol) - mean((all_wines %>% filter(quality < 9))$alcohol)
```

In markdown files, multiple coding languages can be used and objects can be shared between these environments. In this section, I used R to filter the all_wines dataset used throughout this project to only the top-rated wines by quality and created a new dataframe of only these few instances. I accessed this dataframe in Python, and printed out all of the values for each characteristic of the wine using a loop. I saved the lists from some of these instances to python variable objects. 

Back in R, I accessed the variables created in Python and took the means of each of the lists of values. Addtionally, I found the difference in alcohol content in the top-rated wines and the rest of the white wines, and average alcohol content difference between the top-rated wines and all the rest of the wines, both red and white. The top-rated wines have, on average, a 1.67% higher alcohol content than all other white wines and 1.69% higher alcohol content than all other lower-rated wines. I did this same valuation on pH, finding that, on average, the top-rated wines had a 0.12 higher pH than lower-rated white wines and a 0.09 higher pH than all other lower-rated wines.


### Concluding Remarks

While I had hoped that the analyses conducted in this project would indicated the chemical qualities of a high vs. low quality wine or of a red vs. white wine, I didn't find that to be the case. Retrospectively, I should have created an additional binary variable that dictated a threshold between high vs. low quality wine from the quality column. I found the machine learning portions of this project to be intriguing, and I was surprised that linear classifier created a better initial model (less overfitting on the model trained on the entire dataset) than the non-parametric whole-dataset-trained model. I wish that we had covered more of using Python and R interchangeably in this course so the last section of this project could have been more involved. I would have liked to have tapped into my Python knowledge more and demonstrated that here, but I struggled with getting Python to work as I wanted in the markdown file.

